{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Analyzer: Predict & Classify Any Dataset\n",
    "\n",
    "This notebook implements a machine learning analyzer that can perform classification, regression, and clustering tasks on datasets. The implementation includes preprocessing, model training, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "First, we import all the necessary libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration\n",
    "\n",
    "This section handles loading the dataset and exploring its basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load a CSV dataset and return a pandas DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset here\n",
    "# Replace 'your_dataset.csv' with your actual file path\n",
    "df = load_dataset('your_dataset.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "if df is not None:\n",
    "    print(\"\\nDataset Preview:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nDataset Information:\")\n",
    "    display(df.info())\n",
    "    \n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "This section handles data preprocessing, including handling missing values and encoding categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess data with label encoding for categorical features\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return None, {}\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    processed_df = df.copy()\n",
    "    label_encoders = {}\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in processed_df.columns:\n",
    "        if processed_df[col].dtype in ['int64', 'float64']:\n",
    "            # Fill numeric missing values with mean\n",
    "            processed_df[col] = processed_df[col].fillna(processed_df[col].mean())\n",
    "        else:\n",
    "            # Fill categorical missing values with mode\n",
    "            processed_df[col] = processed_df[col].fillna(processed_df[col].mode()[0])\n",
    "    \n",
    "    # Label encode categorical columns\n",
    "    for col in processed_df.columns:\n",
    "        if processed_df[col].dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            processed_df[col] = le.fit_transform(processed_df[col])\n",
    "            label_encoders[col] = le\n",
    "            print(f\"Encoded column '{col}' with {len(le.classes_)} unique values\")\n",
    "    \n",
    "    return processed_df, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "if df is not None:\n",
    "    processed_df, label_encoders = preprocess_data(df)\n",
    "    \n",
    "    print(\"\\nPreprocessed Dataset Preview:\")\n",
    "    display(processed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature and Target Selection\n",
    "\n",
    "This section handles selecting features and target variables for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_target(df, target_col=None, task=\"classification\"):\n",
    "    \"\"\"\n",
    "    Select features and target based on the task\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return None, None\n",
    "    \n",
    "    if task == \"clustering\":\n",
    "        # For clustering, all columns are features\n",
    "        feature_cols = list(df.columns)\n",
    "        return df[feature_cols], None\n",
    "    else:\n",
    "        # For classification and regression, we need a target column\n",
    "        if target_col is None or target_col not in df.columns:\n",
    "            print(\"Error: Please specify a valid target column\")\n",
    "            return None, None\n",
    "        \n",
    "        feature_cols = [col for col in df.columns if col != target_col]\n",
    "        return df[feature_cols], df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your task and target column\n",
    "# Options for task: \"classification\", \"regression\", \"clustering\"\n",
    "task = \"classification\"  # Change this to your desired task\n",
    "target_col = \"Subscribed\"  # Change this to your target column name\n",
    "\n",
    "if processed_df is not None:\n",
    "    X, y = select_features_target(processed_df, target_col, task)\n",
    "    \n",
    "    if X is not None:\n",
    "        print(f\"Selected {len(X.columns)} features for {task} task\")\n",
    "        if task != \"clustering\":\n",
    "            print(f\"Target column: {target_col}\")\n",
    "            if task == \"classification\":\n",
    "                print(f\"Target distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification Models\n",
    "\n",
    "This section implements and evaluates classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_model(X, y, algorithm=\"KNN\"):\n",
    "    \"\"\"\n",
    "    Train a classification model based on the selected algorithm\n",
    "    \"\"\"\n",
    "    if X is None or y is None:\n",
    "        return None\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}\")\n",
    "    \n",
    "    # Train model based on selected algorithm\n",
    "    if algorithm == \"KNN\":\n",
    "        model = KNeighborsClassifier(n_neighbors=3)\n",
    "        print(\"Using KNN classifier with n_neighbors=3\")\n",
    "    elif algorithm == \"SVM\":\n",
    "        model = svm.SVC(kernel='rbf')\n",
    "        print(\"Using SVM classifier with rbf kernel\")\n",
    "    else:  # Decision Tree\n",
    "        model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "        print(\"Using Decision Tree classifier with entropy criterion and max_depth=3\")\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model trained successfully\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"\\nClassification Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "    \n",
    "    # Plot confusion matrix heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "    \n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, X_test, y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classification model if task is classification\n",
    "if task == \"classification\" and X is not None and y is not None:\n",
    "    # Choose algorithm: \"KNN\", \"SVM\", or \"Decision Tree\"\n",
    "    algorithm = \"Decision Tree\"  # Change this to your desired algorithm\n",
    "    \n",
    "    classification_model, X_test, y_test, y_pred = train_classification_model(X, y, algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regression Models\n",
    "\n",
    "This section implements and evaluates regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression_model(X, y):\n",
    "    \"\"\"\n",
    "    Train a regression model\n",
    "    \"\"\"\n",
    "    if X is None or y is None:\n",
    "        return None\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}\")\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    print(\"Using Linear Regression model\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model trained successfully\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"\\nRegression Metrics:\")\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "    print(f\"RÂ² Score: {r2:.3f}\")\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    df_compare = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "    print(f\"\\nActual vs Predicted (first 5 rows):\")\n",
    "    display(df_compare.head())\n",
    "    \n",
    "    # Plot scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Actual vs Predicted Scatter Plot\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, X_test, y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regression model if task is regression\n",
    "if task == \"regression\" and X is not None and y is not None:\n",
    "    regression_model, X_test, y_test, y_pred = train_regression_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clustering Models\n",
    "\n",
    "This section implements and evaluates clustering models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clustering_model(X, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Train a KMeans clustering model\n",
    "    \"\"\"\n",
    "    if X is None:\n",
    "        return None\n",
    "    \n",
    "    # Select first two numeric columns for visualization\n",
    "    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns[:2]\n",
    "    if len(numeric_cols) < 2:\n",
    "        print(\"Error: Need at least 2 numeric columns for clustering visualization\")\n",
    "        return None\n",
    "    \n",
    "    X_cluster = X[numeric_cols]\n",
    "    print(f\"Using columns {numeric_cols[0]} and {numeric_cols[1]} for clustering visualization\")\n",
    "    \n",
    "    # Train KMeans model\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X_cluster)\n",
    "    print(\"KMeans model trained successfully\")\n",
    "    \n",
    "    # Display info\n",
    "    print(f\"\\nKMeans Clustering:\")\n",
    "    print(f\"Number of clusters: {n_clusters}\")\n",
    "    print(f\"Cluster centers:\\n{kmeans.cluster_centers_}\")\n",
    "    \n",
    "    # Count samples in each cluster\n",
    "    unique, counts = np.unique(clusters, return_counts=True)\n",
    "    for i, (cluster, count) in enumerate(zip(unique, counts)):\n",
    "        print(f\"Cluster {cluster}: {count} samples\")\n",
    "    \n",
    "    # Plot clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = X_cluster[clusters == i]\n",
    "        plt.scatter(cluster_points.iloc[:, 0], cluster_points.iloc[:, 1], label=f'Cluster {i}')\n",
    "    \n",
    "    # Plot centroids\n",
    "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "              s=200, marker='*', c='red', label='Centroids')\n",
    "    \n",
    "    plt.xlabel(numeric_cols[0])\n",
    "    plt.ylabel(numeric_cols[1])\n",
    "    plt.title(\"KMeans Clustering\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return kmeans, clusters, X_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train clustering model if task is clustering\n",
    "if task == \"clustering\" and X is not None:\n",
    "    n_clusters = 3  # Change this to your desired number of clusters\n",
    "    clustering_model, clusters, X_cluster = train_clustering_model(X, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Random Data Generation and Prediction\n",
    "\n",
    "This section generates random data and makes predictions using the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_data(df, feature_cols, label_encoders):\n",
    "    \"\"\"\n",
    "    Generate random data based on feature columns\n",
    "    \"\"\"\n",
    "    if df is None or not feature_cols:\n",
    "        print(\"Error: No dataset or feature columns available\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Generate random data based on feature columns\n",
    "        random_data = {}\n",
    "        random_data_original = {}\n",
    "        \n",
    "        for feature in feature_cols:\n",
    "            if feature in df.columns:\n",
    "                if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "                    min_val = df[feature].min()\n",
    "                    max_val = df[feature].max()\n",
    "                    \n",
    "                    if pd.api.types.is_integer_dtype(df[feature]):\n",
    "                        value = np.random.randint(int(min_val), int(max_val) + 1)\n",
    "                    else:\n",
    "                        value = np.random.uniform(min_val, max_val)\n",
    "                    \n",
    "                    random_data[feature] = value\n",
    "                    random_data_original[feature] = value\n",
    "        \n",
    "        # Display random data\n",
    "        print(\"Generated Random Data:\")\n",
    "        for feature, value in random_data.items():\n",
    "            print(f\"{feature}: {value}\")\n",
    "        \n",
    "        return random_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating random data: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data\n",
    "if processed_df is not None and X is not None:\n",
    "    feature_cols = X.columns\n",
    "    random_data = generate_random_data(processed_df, feature_cols, label_encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, random_data, task):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained model\n",
    "    \"\"\"\n",
    "    if model is None or random_data is None:\n",
    "        print(\"Error: No model or random data available\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Create input dataframe from random data\n",
    "        input_df = pd.DataFrame([random_data])\n",
    "        \n",
    "        if task == \"classification\":\n",
    "            # Make prediction\n",
    "            prediction = model.predict(input_df)\n",
    "            \n",
    "            # Simplified output\n",
    "            pred_value = prediction[0]\n",
    "            if pred_value == 1 or pred_value == True:\n",
    "                result = \"Yes\"\n",
    "            else:\n",
    "                result = \"No\"\n",
    "            \n",
    "            print(f\"\\nPrediction: {result}\")\n",
    "            \n",
    "        elif task == \"regression\":\n",
    "            # Make prediction\n",
    "            prediction = model.predict(input_df)\n",
    "            print(f\"\\nPredicted value: {prediction[0]:.2f}\")\n",
    "            \n",
    "        else:  # clustering\n",
    "            # Make prediction\n",
    "            cluster = model.predict(input_df)\n",
    "            print(f\"\\nPredicted cluster: {cluster[0]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error making prediction: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction based on the task\n",
    "if random_data is not None:\n",
    "    if task == \"classification\" and 'classification_model' in locals():\n",
    "        predict(classification_model, random_data, task)\n",
    "    elif task == \"regression\" and 'regression_model' in locals():\n",
    "        predict(regression_model, random_data, task)\n",
    "    elif task == \"clustering\" and 'clustering_model' in locals():\n",
    "        predict(clustering_model, random_data, task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusion\n",
    "\n",
    "This notebook has implemented a complete ML Analyzer that can:\n",
    "\n",
    "1. Load and preprocess datasets\n",
    "2. Handle missing values and encode categorical features\n",
    "3. Perform classification tasks with KNN, SVM, or Decision Tree algorithms\n",
    "4. Perform regression tasks with Linear Regression\n",
    "5. Perform clustering tasks with KMeans\n",
    "6. Generate random data for prediction\n",
    "7. Make predictions and visualize results\n",
    "\n",
    "To use this notebook with your own dataset:\n",
    "1. Update the file path in the data loading section\n",
    "2. Select your target column and task type\n",
    "3. Choose the appropriate algorithm\n",
    "4. Run the cells in sequence\n",
    "\n",
    "The notebook provides a modular approach to machine learning tasks, making it easy to adapt to different datasets and requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
